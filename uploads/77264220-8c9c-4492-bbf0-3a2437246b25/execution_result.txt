
[2025-08-13 17:28:49]
üìú Executing Code:
import pandas as pd
import os
import re
import io

# Create the directory if it doesn't exist
output_dir = "uploads/77264220-8c9c-4492-bbf0-3a2437246b25"
os.makedirs(output_dir, exist_ok=True)

# URL of the Wikipedia page
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Read HTML tables into a list of DataFrames
# pandas.read_html can directly parse tables from a URL
tables = pd.read_html(url)

# The first table usually contains the main list of highest-grossing films
df = tables[0].copy()


# Function to clean numeric columns by removing non-digit characters and converting to numeric
def clean_numeric_column(series):
    # Remove bracketed references (e.g., [A], [2]), then non-digit characters
    cleaned = series.astype(str).str.replace(r"\[.*?\]", "", regex=True)
    return (
        cleaned.str.replace(r"[^0-9]", "", regex=True)
        .replace("", pd.NA)
        .astype("Int64")
    )


# Function to clean currency column
def clean_currency_column(series):
    # Remove '$', ',', and any bracketed references like '[A]', '[B]', '[1]'
    cleaned = series.astype(str).str.replace("$", "", regex=False)
    cleaned = cleaned.str.replace(",", "", regex=False)
    cleaned = cleaned.str.replace(r"\[.*?\]", "", regex=True)
    return cleaned.replace("", pd.NA).astype(float)


# Apply cleaning to relevant columns
if "Rank" in df.columns:
    df["Rank"] = clean_numeric_column(df["Rank"])
if "Peak" in df.columns:
    df["Peak"] = clean_numeric_column(df["Peak"])
if "Worldwide gross" in df.columns:
    df["Worldwide gross"] = clean_currency_column(df["Worldwide gross"])
if "Year" in df.columns:
    df["Year"] = clean_numeric_column(df["Year"])

# Save the cleaned DataFrame to data.csv
output_csv_path = os.path.join(output_dir, "data.csv")
df.to_csv(output_csv_path, index=False)

# Generate metadata
metadata_path = os.path.join(output_dir, "metadata.txt")

# Capture df.info() output
buffer = io.StringIO()
df.info(buf=buffer)
info_output = buffer.getvalue()
buffer.close()

with open(metadata_path, "w") as f:
    f.write("Output of df.info():\n")
    f.write(info_output)
    f.write("\n\nColumn names:\n")
    f.write(str(df.columns.tolist()))
    f.write("\n\nFirst few rows (df.head()):\n")
    f.write(df.head().to_string())
    f.write("\n\nANSWER_FORMAT: JSON\n")

----------------------------------------

[2025-08-13 17:28:51]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 40, in <module>
  File "<string>", line 32, in clean_currency_column
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/generic.py", line 6662, in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py", line 430, in astype
    return self.apply(
           ^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py", line 363, in apply
    applied = getattr(b, f)(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py", line 784, in astype
    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py", line 237, in astype_array_safe
    new_values = astype_array(values, dtype, copy=copy)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py", line 182, in astype_array
    values = _astype_nansafe(values, dtype, copy=copy)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/charmika/TDS_project2/venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py", line 133, in _astype_nansafe
    return arr.astype(dtype, copy=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: could not convert string to float: 'T2257844554'

----------------------------------------

[2025-08-13 17:29:37]
üìú Executing Code:
import pandas as pd
import requests
import os
import io

# Define paths
output_dir = "uploads/77264220-8c9c-4492-bbf0-3a2437246b25"
data_csv_path = os.path.join(output_dir, "data.csv")
metadata_txt_path = os.path.join(output_dir, "metadata.txt")

# Create output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# URL of the Wikipedia page
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Fetch the HTML content
response = requests.get(url)
response.raise_for_status()  # Raise an exception for bad status codes

# Read tables into a list of DataFrames
tables = pd.read_html(response.text)

# Identify the correct table.
# The main table of films is usually the first large table.
# It typically has columns like 'No.', 'Film', 'Worldwide gross', 'Year', 'Peak worldwide gross'.
main_df = None
for df_candidate in tables:
    # Normalize column names to lower case and remove extra spaces/newlines for initial check
    normalized_cols_candidate = [
        col.strip().replace("\n", " ").lower() for col in df_candidate.columns
    ]

    # Look for key columns: 'film', 'worldwide gross', 'year'
    if all(
        col_name in normalized_cols_candidate
        for col_name in ["film", "worldwide gross", "year"]
    ):
        # Also check if it's a reasonably large table (e.g., more than 50 rows)
        if len(df_candidate) > 50:
            main_df = df_candidate
            break

if main_df is None:
    # Fallback: if auto-identification fails, pick the first table.
    # On this specific Wikipedia page, the first table is generally the correct one.
    main_df = tables[0]

# Clean column names: remove numbers/text in brackets (like "[1]", "[a]", "[note 1]"), newlines, and strip whitespace
# Also remove trailing numbers like 'Worldwide gross.1' if merge occurred
original_cols = main_df.columns.tolist()
new_cols = []
for col in original_cols:
    cleaned_col = str(col).strip().replace("\n", " ")
    # Remove bracketed references and trailing numbers from merged cells (like 'Worldwide gross.1')
    cleaned_col = (
        pd.Series([cleaned_col]).str.replace(r"\[.*?\]|\.\d+", "", regex=True).iloc[0]
    )
    cleaned_col = cleaned_col.strip()
    new_cols.append(cleaned_col)
main_df.columns = new_cols


# Standardize column names for easier access
# Use a dictionary to map possible scraped column names to standard names
standard_column_map = {
    "No.": "Rank",
    "Film": "Film",
    "Worldwide gross": "Worldwide gross",
    "Year": "Year",
    "Peak worldwide gross": "Peak",
    "Worldwide Gross": "Worldwide gross",
    "Release year": "Year",
    "Peak Worldwide Gross": "Peak",
    "Rank": "Rank",
}

# Create a mapping from current DataFrame columns to standard names
rename_dict = {}
for current_col in main_df.columns:
    for scraped_name, standard_name in standard_column_map.items():
        if (
            scraped_name.lower() == current_col.lower()
        ):  # Case-insensitive exact match for standardization
            rename_dict[current_col] = standard_name
            break
# Apply the renaming
df_final = main_df.rename(columns=rename_dict)

# Select only the relevant columns for analysis
required_analysis_cols = ["Rank", "Film", "Worldwide gross", "Year", "Peak"]
# Filter out columns that don't exist after renaming to prevent KeyError
actual_cols_to_keep = [col for col in required_analysis_cols if col in df_final.columns]
df_final = df_final[actual_cols_to_keep].copy()

# Data Cleaning for numeric columns
# 'Worldwide gross' and 'Peak': Remove '$', ',', ' ', 'T', and any bracketed text, then convert to numeric.
# 'Rank': Remove any non-numeric text, convert to numeric.
# 'Year': Extract 4-digit year, convert to numeric.

for col in ["Worldwide gross", "Peak"]:
    if col in df_final.columns:
        # Convert to string, replace common non-numeric chars and references
        df_final[col] = (
            df_final[col]
            .astype(str)
            .str.replace("$", "", regex=False)
            .str.replace(",", "", regex=False)
            .str.replace(" ", "", regex=False)
            .str.replace("T", "", regex=False)
            .str.replace(r"\[.*?\]", "", regex=True)
        )  # Remove [any_text]
        # Convert to numeric, coercing errors to NaN
        df_final[col] = pd.to_numeric(df_final[col], errors="coerce")

if "Rank" in df_final.columns:
    df_final["Rank"] = (
        df_final["Rank"].astype(str).str.replace(r"\[.*?\]", "", regex=True)
    )  # Remove [any_text]
    df_final["Rank"] = pd.to_numeric(df_final["Rank"], errors="coerce")

if "Year" in df_final.columns:
    # Extract only 4-digit numbers as year, then convert to nullable integer
    df_final["Year"] = (
        df_final["Year"].astype(str).str.extract(r"(\d{4})").astype(float)
    )
    df_final["Year"] = pd.to_numeric(df_final["Year"], errors="coerce").astype(
        "Int64"
    )  # Use Int64 for nullable integer


# Drop rows where critical columns for analysis are NaN (e.g., Worldwide gross, Year, Rank)
df_final.dropna(subset=["Rank", "Worldwide gross", "Year"], inplace=True)


# Save the cleaned DataFrame to CSV
df_final.to_csv(data_csv_path, index=False)

# Generate metadata
metadata_content = io.StringIO()
metadata_content.write("Output of df.info():\n")
df_final.info(buf=metadata_content)
metadata_content.write("\n\nColumn names:\n")
metadata_content.write(str(df_final.columns.tolist()))
metadata_content.write("\n\nFirst few rows (df.head()):\n")
metadata_content.write(df_final.head().to_string())
metadata_content.write("\n\nANSWER_FORMAT: JSON array of strings")

# Save metadata to file
with open(metadata_txt_path, "w") as f:
    f.write(metadata_content.getvalue())

----------------------------------------

[2025-08-13 17:29:38]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 30, in <module>
AttributeError: 'int' object has no attribute 'strip'

----------------------------------------

[2025-08-13 17:30:07]
üìú Executing Code:
import pandas as pd
import requests
import os
import re

# Create the directory for uploads if it doesn't exist
output_dir = "uploads/77264220-8c9c-4492-bbf0-3a2437246b25"
os.makedirs(output_dir, exist_ok=True)

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Use requests to get the HTML content
response = requests.get(url)
response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)

# Use pandas.read_html to parse tables
tables = pd.read_html(response.text)

df = None
# Iterate through the tables to find the one with film data
# Look for a table that contains 'Film', 'Worldwide gross', 'Year', 'Rank', and 'Peak'
# The main table usually has these columns and is one of the first few tables.
for table in tables:
    if all(
        col in table.columns
        for col in ["Film", "Worldwide gross", "Year", "Rank", "Peak"]
    ):
        df = table
        break

if df is None:
    raise ValueError(
        "Could not find the expected table on the Wikipedia page with required columns."
    )

# Data Cleaning
# Drop columns that are not relevant (e.g., 'Reference(s)' or similar)
cols_to_drop = [col for col in df.columns if "Reference" in col or "Ref" in col]
df = df.drop(columns=cols_to_drop, errors="ignore")

# Clean 'Worldwide gross' column
df["Worldwide gross"] = df["Worldwide gross"].astype(
    str
)  # Ensure string type for operations
df["Worldwide gross"] = df["Worldwide gross"].str.replace("$", "", regex=False)
df["Worldwide gross"] = df["Worldwide gross"].str.replace(",", "", regex=False)
df["Worldwide gross"] = (
    df["Worldwide gross"].apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
)
df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean 'Year' column
df["Year"] = df["Year"].astype(str)  # Ensure string type
df["Year"] = df["Year"].apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
df["Year"] = pd.to_numeric(df["Year"], errors="coerce").astype(
    "Int64"
)  # Use nullable integer

# Clean 'Rank' column
df["Rank"] = df["Rank"].astype(str)  # Ensure string type
df["Rank"] = df["Rank"].apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce").astype("Int64")

# Clean 'Peak' column
df["Peak"] = df["Peak"].astype(str)  # Ensure string type
df["Peak"] = df["Peak"].apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce").astype("Int64")

# Drop rows where critical columns (Worldwide gross, Year, Rank, Peak) are NaN after cleaning
df.dropna(subset=["Worldwide gross", "Year", "Rank", "Peak"], inplace=True)

# Convert nullable integers to standard integers after dropping NaNs, if desired
df["Year"] = df["Year"].astype(int)
df["Rank"] = df["Rank"].astype(int)
df["Peak"] = df["Peak"].astype(int)

# Save the final dataset to CSV
output_csv_path = os.path.join(output_dir, "data.csv")
df.to_csv(output_csv_path, index=False)

# Generate metadata
metadata_path = os.path.join(output_dir, "metadata.txt")
with open(metadata_path, "w") as f:
    f.write(f"Output of df.info():\n")
    df.info(buf=f)  # Write info directly to file buffer

    f.write("\n\nColumn names:\n")
    f.write("\n".join(df.columns.tolist()))

    f.write("\n\nFirst few rows (df.head()):\n")
    f.write(df.head().to_string())

    f.write("\n\nANSWER_FORMAT: JSON array of strings")

----------------------------------------

[2025-08-13 17:30:08]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 29, in <module>
ValueError: Could not find the expected table on the Wikipedia page with required columns.

----------------------------------------

[2025-08-13 17:30:18]
üìú Executing Code:
import pandas as pd
import os

# Create the output directory if it does not exist
output_dir = "uploads/77264220-8c9c-4492-bbf0-3a2437246b25"
os.makedirs(output_dir, exist_ok=True)

# URL of the Wikipedia page
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Read all tables from the Wikipedia page
tables = pd.read_html(url)

df = None
# Iterate through the tables to find the one with expected columns
expected_columns_subset = ["Film", "Worldwide gross", "Year", "Rank", "Peak"]
for table in tables:
    # Check if a significant number of expected columns are present
    # The main table might also have 'Ref.' or other columns, so check for a subset.
    if all(
        col in table.columns for col in expected_columns_subset[:3]
    ):  # Check for Film, Worldwide gross, Year initially
        # Additional check for 'Rank' and 'Peak' which are critical for the questions
        if "Rank" in table.columns and "Peak" in table.columns:
            df = table
            break

if df is None:
    raise ValueError(
        "Could not find the expected table on the Wikipedia page with required columns."
    )

# Clean the 'Worldwide gross' column
# Remove '$', ',', and any text within brackets (like references [n])
df["Worldwide gross"] = (
    df["Worldwide gross"]
    .astype(str)
    .str.replace("$", "", regex=False)
    .str.replace(",", "", regex=False)
    .str.replace(r"\[.*\]", "", regex=True)
)
df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean the 'Year' column
df["Year"] = pd.to_numeric(df["Year"], errors="coerce").astype(
    "Int64"
)  # Use 'Int64' to allow NaN for missing years

# Ensure 'Rank' and 'Peak' are numeric
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce").astype("Int64")
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce").astype("Int64")

# Drop rows where 'Worldwide gross' or 'Year' are NaN after conversion, as they are crucial for analysis
df.dropna(subset=["Worldwide gross", "Year", "Rank", "Peak"], inplace=True)

# Save the processed DataFrame to CSV
df.to_csv(os.path.join(output_dir, "data.csv"), index=False)

# Generate metadata
metadata_path = os.path.join(output_dir, "metadata.txt")
with open(metadata_path, "w") as f:
    f.write(
        df.info(buf=None, verbose=True, show_counts=True).to_string() + "\n\n"
    )  # Capture df.info() output
    f.write("Column names:\n")
    for col in df.columns:
        f.write(f"- {col}\n")
    f.write("\nFirst few rows:\n")
    f.write(df.head().to_string() + "\n\n")
    f.write("ANSWER_FORMAT: JSON\n")

----------------------------------------

[2025-08-13 17:30:20]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 27, in <module>
ValueError: Could not find the expected table on the Wikipedia page with required columns.

----------------------------------------
