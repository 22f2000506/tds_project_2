{
    "code": "import pandas as pd\nimport os\nimport re\nimport io\n\n# Create the directory if it doesn't exist\noutput_dir = \"uploads/77264220-8c9c-4492-bbf0-3a2437246b25\"\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Read HTML tables into a list of DataFrames\n# pandas.read_html can directly parse tables from a URL\ntables = pd.read_html(url)\n\n# The first table usually contains the main list of highest-grossing films\ndf = tables[0].copy()\n\n# Function to clean numeric columns by removing non-digit characters and converting to numeric\ndef clean_numeric_column(series):\n    # Remove bracketed references (e.g., [A], [2]), then non-digit characters\n    cleaned = series.astype(str).str.replace(r'\\[.*?\\]', '', regex=True)\n    return cleaned.str.replace(r'[^0-9]', '', regex=True).replace('', pd.NA).astype('Int64')\n\n# Function to clean currency column\ndef clean_currency_column(series):\n    # Remove '$', ',', and any bracketed references like '[A]', '[B]', '[1]'\n    cleaned = series.astype(str).str.replace('$', '', regex=False)\n    cleaned = cleaned.str.replace(',', '', regex=False)\n    cleaned = cleaned.str.replace(r'\\[.*?\\]', '', regex=True)\n    return cleaned.replace('', pd.NA).astype(float)\n\n# Apply cleaning to relevant columns\nif 'Rank' in df.columns:\n    df['Rank'] = clean_numeric_column(df['Rank'])\nif 'Peak' in df.columns:\n    df['Peak'] = clean_numeric_column(df['Peak'])\nif 'Worldwide gross' in df.columns:\n    df['Worldwide gross'] = clean_currency_column(df['Worldwide gross'])\nif 'Year' in df.columns:\n    df['Year'] = clean_numeric_column(df['Year'])\n\n# Save the cleaned DataFrame to data.csv\noutput_csv_path = os.path.join(output_dir, \"data.csv\")\ndf.to_csv(output_csv_path, index=False)\n\n# Generate metadata\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Capture df.info() output\nbuffer = io.StringIO()\ndf.info(buf=buffer)\ninfo_output = buffer.getvalue()\nbuffer.close()\n\nwith open(metadata_path, \"w\") as f:\n    f.write(\"Output of df.info():\\n\")\n    f.write(info_output)\n    f.write(\"\\n\\nColumn names:\\n\")\n    f.write(str(df.columns.tolist()))\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\nANSWER_FORMAT: JSON\\n\")\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\n\n# Define paths\noutput_dir = \"uploads/77264220-8c9c-4492-bbf0-3a2437246b25\"\ndata_csv_path = os.path.join(output_dir, \"data.csv\")\nmetadata_txt_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch the HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an exception for bad status codes\n\n# Read tables into a list of DataFrames\ntables = pd.read_html(response.text)\n\n# Identify the correct table.\n# The main table of films is usually the first large table.\n# It typically has columns like 'No.', 'Film', 'Worldwide gross', 'Year', 'Peak worldwide gross'.\nmain_df = None\nfor df_candidate in tables:\n    # Normalize column names to lower case and remove extra spaces/newlines for initial check\n    normalized_cols_candidate = [col.strip().replace('\\n', ' ').lower() for col in df_candidate.columns]\n\n    # Look for key columns: 'film', 'worldwide gross', 'year'\n    if all(col_name in normalized_cols_candidate for col_name in ['film', 'worldwide gross', 'year']):\n        # Also check if it's a reasonably large table (e.g., more than 50 rows)\n        if len(df_candidate) > 50:\n            main_df = df_candidate\n            break\n\nif main_df is None:\n    # Fallback: if auto-identification fails, pick the first table.\n    # On this specific Wikipedia page, the first table is generally the correct one.\n    main_df = tables[0]\n\n# Clean column names: remove numbers/text in brackets (like \"[1]\", \"[a]\", \"[note 1]\"), newlines, and strip whitespace\n# Also remove trailing numbers like 'Worldwide gross.1' if merge occurred\noriginal_cols = main_df.columns.tolist()\nnew_cols = []\nfor col in original_cols:\n    cleaned_col = str(col).strip().replace('\\n', ' ')\n    # Remove bracketed references and trailing numbers from merged cells (like 'Worldwide gross.1')\n    cleaned_col = pd.Series([cleaned_col]).str.replace(r'\\[.*?\\]|\\.\\d+', '', regex=True).iloc[0]\n    cleaned_col = cleaned_col.strip()\n    new_cols.append(cleaned_col)\nmain_df.columns = new_cols\n\n\n# Standardize column names for easier access\n# Use a dictionary to map possible scraped column names to standard names\nstandard_column_map = {\n    'No.': 'Rank',\n    'Film': 'Film',\n    'Worldwide gross': 'Worldwide gross',\n    'Year': 'Year',\n    'Peak worldwide gross': 'Peak',\n    'Worldwide Gross': 'Worldwide gross',\n    'Release year': 'Year',\n    'Peak Worldwide Gross': 'Peak',\n    'Rank': 'Rank'\n}\n\n# Create a mapping from current DataFrame columns to standard names\nrename_dict = {}\nfor current_col in main_df.columns:\n    for scraped_name, standard_name in standard_column_map.items():\n        if scraped_name.lower() == current_col.lower(): # Case-insensitive exact match for standardization\n            rename_dict[current_col] = standard_name\n            break\n# Apply the renaming\ndf_final = main_df.rename(columns=rename_dict)\n\n# Select only the relevant columns for analysis\nrequired_analysis_cols = ['Rank', 'Film', 'Worldwide gross', 'Year', 'Peak']\n# Filter out columns that don't exist after renaming to prevent KeyError\nactual_cols_to_keep = [col for col in required_analysis_cols if col in df_final.columns]\ndf_final = df_final[actual_cols_to_keep].copy()\n\n# Data Cleaning for numeric columns\n# 'Worldwide gross' and 'Peak': Remove '$', ',', ' ', 'T', and any bracketed text, then convert to numeric.\n# 'Rank': Remove any non-numeric text, convert to numeric.\n# 'Year': Extract 4-digit year, convert to numeric.\n\nfor col in ['Worldwide gross', 'Peak']:\n    if col in df_final.columns:\n        # Convert to string, replace common non-numeric chars and references\n        df_final[col] = df_final[col].astype(str) \\\n                                   .str.replace('$', '', regex=False) \\\n                                   .str.replace(',', '', regex=False) \\\n                                   .str.replace(' ', '', regex=False) \\\n                                   .str.replace('T', '', regex=False) \\\n                                   .str.replace(r'\\[.*?\\]', '', regex=True) # Remove [any_text]\n        # Convert to numeric, coercing errors to NaN\n        df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n\nif 'Rank' in df_final.columns:\n    df_final['Rank'] = df_final['Rank'].astype(str) \\\n                                   .str.replace(r'\\[.*?\\]', '', regex=True) # Remove [any_text]\n    df_final['Rank'] = pd.to_numeric(df_final['Rank'], errors='coerce')\n\nif 'Year' in df_final.columns:\n    # Extract only 4-digit numbers as year, then convert to nullable integer\n    df_final['Year'] = df_final['Year'].astype(str).str.extract(r'(\\d{4})').astype(float)\n    df_final['Year'] = pd.to_numeric(df_final['Year'], errors='coerce').astype('Int64') # Use Int64 for nullable integer\n\n\n# Drop rows where critical columns for analysis are NaN (e.g., Worldwide gross, Year, Rank)\ndf_final.dropna(subset=['Rank', 'Worldwide gross', 'Year'], inplace=True)\n\n\n# Save the cleaned DataFrame to CSV\ndf_final.to_csv(data_csv_path, index=False)\n\n# Generate metadata\nmetadata_content = io.StringIO()\nmetadata_content.write(\"Output of df.info():\\n\")\ndf_final.info(buf=metadata_content)\nmetadata_content.write(\"\\n\\nColumn names:\\n\")\nmetadata_content.write(str(df_final.columns.tolist()))\nmetadata_content.write(\"\\n\\nFirst few rows (df.head()):\\n\")\nmetadata_content.write(df_final.head().to_string())\nmetadata_content.write(\"\\n\\nANSWER_FORMAT: JSON array of strings\")\n\n# Save metadata to file\nwith open(metadata_txt_path, \"w\") as f:\n    f.write(metadata_content.getvalue())\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0K...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport re\n\n# Create the directory for uploads if it doesn't exist\noutput_dir = \"uploads/77264220-8c9c-4492-bbf0-3a2437246b25\"\nos.makedirs(output_dir, exist_ok=True)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Use requests to get the HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n\n# Use pandas.read_html to parse tables\ntables = pd.read_html(response.text)\n\ndf = None\n# Iterate through the tables to find the one with film data\n# Look for a table that contains 'Film', 'Worldwide gross', 'Year', 'Rank', and 'Peak'\n# The main table usually has these columns and is one of the first few tables.\nfor table in tables:\n    if all(col in table.columns for col in ['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak']):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page with required columns.\")\n\n# Data Cleaning\n# Drop columns that are not relevant (e.g., 'Reference(s)' or similar)\ncols_to_drop = [col for col in df.columns if 'Reference' in col or 'Ref' in col]\ndf = df.drop(columns=cols_to_drop, errors='ignore')\n\n# Clean 'Worldwide gross' column\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str) # Ensure string type for operations\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace('$', '', regex=False)\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace(',', '', regex=False)\ndf['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean 'Year' column\ndf['Year'] = df['Year'].astype(str) # Ensure string type\ndf['Year'] = df['Year'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64') # Use nullable integer\n\n# Clean 'Rank' column\ndf['Rank'] = df['Rank'].astype(str) # Ensure string type\ndf['Rank'] = df['Rank'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce').astype('Int64')\n\n# Clean 'Peak' column\ndf['Peak'] = df['Peak'].astype(str) # Ensure string type\ndf['Peak'] = df['Peak'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce').astype('Int64')\n\n# Drop rows where critical columns (Worldwide gross, Year, Rank, Peak) are NaN after cleaning\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Convert nullable integers to standard integers after dropping NaNs, if desired\ndf['Year'] = df['Year'].astype(int)\ndf['Rank'] = df['Rank'].astype(int)\ndf['Peak'] = df['Peak'].astype(int)\n\n# Save the final dataset to CSV\noutput_csv_path = os.path.join(output_dir, \"data.csv\")\ndf.to_csv(output_csv_path, index=False)\n\n# Generate metadata\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\nwith open(metadata_path, \"w\") as f:\n    f.write(f\"Output of df.info():\\n\")\n    df.info(buf=f) # Write info directly to file buffer\n\n    f.write(\"\\n\\nColumn names:\\n\")\n    f.write(\"\\n\".join(df.columns.tolist()))\n\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n\n    f.write(\"\\n\\nANSWER_FORMAT: JSON array of strings\")\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0KG...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 1"
}{
    "code": "import pandas as pd\nimport os\n\n# Create the output directory if it does not exist\noutput_dir = 'uploads/77264220-8c9c-4492-bbf0-3a2437246b25'\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\n# Read all tables from the Wikipedia page\ntables = pd.read_html(url)\n\ndf = None\n# Iterate through the tables to find the one with expected columns\nexpected_columns_subset = ['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak']\nfor table in tables:\n    # Check if a significant number of expected columns are present\n    # The main table might also have 'Ref.' or other columns, so check for a subset.\n    if all(col in table.columns for col in expected_columns_subset[:3]): # Check for Film, Worldwide gross, Year initially\n        # Additional check for 'Rank' and 'Peak' which are critical for the questions\n        if 'Rank' in table.columns and 'Peak' in table.columns:\n            df = table\n            break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page with required columns.\")\n\n# Clean the 'Worldwide gross' column\n# Remove '$', ',', and any text within brackets (like references [n])\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False).str.replace(r'\\[.*\\]', '', regex=True)\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean the 'Year' column\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64') # Use 'Int64' to allow NaN for missing years\n\n# Ensure 'Rank' and 'Peak' are numeric\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce').astype('Int64')\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce').astype('Int64')\n\n# Drop rows where 'Worldwide gross' or 'Year' are NaN after conversion, as they are crucial for analysis\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Save the processed DataFrame to CSV\ndf.to_csv(os.path.join(output_dir, 'data.csv'), index=False)\n\n# Generate metadata\nmetadata_path = os.path.join(output_dir, 'metadata.txt')\nwith open(metadata_path, 'w') as f:\n    f.write(df.info(buf=None, verbose=True, show_counts=True).to_string() + '\\n\\n') # Capture df.info() output\n    f.write('Column names:\\n')\n    for col in df.columns:\n        f.write(f'- {col}\\n')\n    f.write('\\nFirst few rows:\\n')\n    f.write(df.head().to_string() + '\\n\\n')\n    f.write('ANSWER_FORMAT: JSON\\n')\n",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 2"
}