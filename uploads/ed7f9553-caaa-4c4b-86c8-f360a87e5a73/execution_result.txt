
[2025-08-13 17:45:01]
üìú Executing Code:
import pandas as pd
import os
import re
import io

# Define the URL and output paths
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
output_dir = "uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73"
output_csv_path = os.path.join(output_dir, "data.csv")
metadata_path = os.path.join(output_dir, "metadata.txt")

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Read tables from the URL
tables = pd.read_html(url)

# Find the correct table. It should contain 'Film', 'Worldwide gross', 'Year', 'Rank', 'Peak'.
df = None
for table in tables:
    if all(
        col in table.columns
        for col in ["Film", "Worldwide gross", "Year", "Rank", "Peak"]
    ):
        df = table
        break

if df is None:
    raise ValueError("Could not find the expected table on the Wikipedia page.")

# Clean the 'Worldwide gross' column
df["Worldwide gross"] = df["Worldwide gross"].astype(str)
df["Worldwide gross"] = df["Worldwide gross"].apply(
    lambda x: re.sub(r"\[.*?\]", "", x)
)  # Remove [citations]
df["Worldwide gross"] = (
    df["Worldwide gross"]
    .str.replace("$", "", regex=False)
    .str.replace(",", "", regex=False)
)
df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean 'Rank' and 'Peak' columns
# Replace non-numeric values (like '‚Äî') and convert to numeric
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
df["Peak"] = df["Peak"].astype(str).replace("‚Äî", pd.NA)  # Replace em-dash with NA
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

# Ensure 'Year' is numeric
df["Year"] = pd.to_numeric(df["Year"], errors="coerce")

# Save the cleaned DataFrame to CSV
df.to_csv(output_csv_path, index=False)

# Generate metadata
metadata_content = io.StringIO()
metadata_content.write("Output of df.info():\n")
df.info(buf=metadata_content)
metadata_content.write("\n\nColumn names:\n")
metadata_content.write(str(df.columns.tolist()))
metadata_content.write("\n\nFirst few rows (df.head()):\n")
metadata_content.write(str(df.head().to_string()))  # Use to_string to avoid truncation
metadata_content.write("\n\nANSWER_FORMAT:\n")
metadata_content.write(
    "JSON array of strings containing the answer."
)  # As specified in question.txt

with open(metadata_path, "w") as f:
    f.write(metadata_content.getvalue())

----------------------------------------

[2025-08-13 17:45:03]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 26, in <module>
ValueError: Could not find the expected table on the Wikipedia page.

----------------------------------------

[2025-08-13 17:45:30]
üìú Executing Code:
import pandas as pd
import os
import re
import io

# Define the URL and output paths
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
output_dir = "uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73"
output_csv_path = os.path.join(output_dir, "data.csv")
metadata_path = os.path.join(output_dir, "metadata.txt")

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Read tables from the URL
tables = pd.read_html(url)

# Find the correct table. It should contain 'Film', 'Worldwide gross', 'Year', 'Rank', 'Peak'.
df = None
for table in tables:
    if all(
        col in table.columns
        for col in ["Film", "Worldwide gross", "Year", "Rank", "Peak"]
    ):
        df = table
        break

if df is None:
    raise ValueError("Could not find the expected table on the Wikipedia page.")

# Clean the 'Worldwide gross' column
df["Worldwide gross"] = df["Worldwide gross"].astype(str)
df["Worldwide gross"] = df["Worldwide gross"].apply(
    lambda x: re.sub(r"\[.*?\]", "", x)
)  # Remove [citations]
df["Worldwide gross"] = (
    df["Worldwide gross"]
    .str.replace("$", "", regex=False)
    .str.replace(",", "", regex=False)
)
df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean 'Rank' and 'Peak' columns
# Replace non-numeric values (like '‚Äî') and convert to numeric
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
df["Peak"] = df["Peak"].astype(str).replace("‚Äî", pd.NA)  # Replace em-dash with NA
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

# Ensure 'Year' is numeric
df["Year"] = pd.to_numeric(df["Year"], errors="coerce")

# Save the cleaned DataFrame to CSV
df.to_csv(output_csv_path, index=False)

# Generate metadata
metadata_content = io.StringIO()
metadata_content.write("Output of df.info():\n")
df.info(buf=metadata_content)
metadata_content.write("\n\nColumn names:\n")
metadata_content.write(str(df.columns.tolist()))
metadata_content.write("\n\nFirst few rows (df.head()):\n")
metadata_content.write(str(df.head().to_string()))  # Use to_string to avoid truncation
metadata_content.write("\n\nANSWER_FORMAT:\n")
metadata_content.write(
    "JSON array of strings containing the answer."
)  # As specified in question.txt

with open(metadata_path, "w") as f:
    f.write(metadata_content.getvalue())

----------------------------------------

[2025-08-13 17:45:32]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 26, in <module>
ValueError: Could not find the expected table on the Wikipedia page.

----------------------------------------

[2025-08-13 17:46:07]
üìú Executing Code:
import pandas as pd
import requests
import os
import re
from io import StringIO

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
output_dir = "uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73"

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Fetch HTML content
try:
    response = requests.get(url)
    response.raise_for_status()  # Raise an exception for bad status codes
except requests.exceptions.RequestException as e:
    raise SystemExit(f"Failed to retrieve the webpage: {e}")

# Use pandas to read HTML tables
# It returns a list of dataframes found on the page
list_of_dfs = pd.read_html(response.text)

df = None
# Iterate through the list of dataframes to find the correct one
# The main table should have 'Film' and 'Worldwide gross' columns
for temp_df in list_of_dfs:
    if "Film" in temp_df.columns and "Worldwide gross" in temp_df.columns:
        df = temp_df
        break

if df is None:
    raise ValueError(
        "Could not find the expected table on the Wikipedia page with 'Film' and 'Worldwide gross' columns."
    )

# Clean the DataFrame
# Remove 'Reference(s)' column if it exists and is not relevant for analysis
if "Reference(s)" in df.columns:
    df = df.drop(columns=["Reference(s)"])

# Clean 'Worldwide gross' column
# Remove '$', ',', citation brackets (e.g., [1]), and anything in parentheses
df["Worldwide gross"] = (
    df["Worldwide gross"].astype(str).apply(lambda x: re.sub(r"[$,\[\]]", "", x))
)
df["Worldwide gross"] = df["Worldwide gross"].apply(
    lambda x: re.sub(r"\s*\([^)]*\)", "", x)
)
# Convert to numeric, coercing errors to NaN
df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean 'Year' column
# Extract the first four digits as the year
df["Year"] = (
    df["Year"]
    .astype(str)
    .apply(
        lambda x: re.search(r"\d{4}", x).group(0) if re.search(r"\d{4}", x) else None
    )
)
# Convert to numeric (nullable integer) after cleaning
df["Year"] = pd.to_numeric(df["Year"], errors="coerce").astype("Int64")

# Clean 'Rank' column
# Convert to numeric, coercing errors to NaN, then to nullable integer
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce").astype("Int64")

# Drop rows where essential columns are NaN after cleaning (e.g., header rows, invalid data)
df.dropna(subset=["Rank", "Worldwide gross", "Year", "Film"], inplace=True)

# Ensure data types are correct after cleaning and dropping NaNs for non-nullable use
df["Rank"] = df["Rank"].astype(int)
df["Year"] = df["Year"].astype(int)

# Sort by Rank to ensure consistent order
df = df.sort_values(by="Rank").reset_index(drop=True)

# Save the DataFrame to CSV
csv_path = os.path.join(output_dir, "data.csv")
df.to_csv(csv_path, index=False)

# Generate metadata.txt
metadata_path = os.path.join(output_dir, "metadata.txt")
with open(metadata_path, "w") as f:
    f.write("Output of df.info():\n")
    buffer = StringIO()
    df.info(buf=buffer)
    f.write(buffer.getvalue())
    f.write("\n\nColumn names:\n")
    f.write(str(df.columns.tolist()))
    f.write("\n\nFirst few rows (df.head()):\n")
    f.write(df.head().to_string())  # Use .to_string() for better formatting in file
    f.write("\n\nANSWER_FORMAT:\n")
    f.write("JSON array of strings containing the answer.")

----------------------------------------

[2025-08-13 17:46:08]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 33, in <module>
ValueError: Could not find the expected table on the Wikipedia page with 'Film' and 'Worldwide gross' columns.

----------------------------------------

[2025-08-13 17:51:07]
üìú Executing Code:
import pandas as pd
import requests
import os
import io
import re

# Define paths
OUTPUT_DIR = "uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73"
DATA_FILE_PATH = os.path.join(OUTPUT_DIR, "data.csv")
METADATA_FILE_PATH = os.path.join(OUTPUT_DIR, "metadata.txt")
URL = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Fetch HTML content
response = requests.get(URL)
response.raise_for_status()  # Raise an exception for bad status codes

# Use pandas to read HTML tables
# It returns a list of DataFrames found on the page
tables = pd.read_html(response.text)

df = None
# Iterate through tables to find the one with the expected columns
# The column names might vary slightly, e.g., 'Film' or 'Title', 'Worldwide gross', 'Year', 'Rank', 'Peak'
expected_col_keywords = ["film", "worldwide gross"]

for current_df in tables:
    # Normalize column names for easier checking (case-insensitive)
    normalized_cols = [col.lower() for col in current_df.columns]

    # Check if key columns are present
    if all(keyword in normalized_cols for keyword in expected_col_keywords):
        df = current_df
        # Standardize column names if variations exist (e.g., 'Title' to 'Film')
        if "Title" in df.columns and "Film" not in df.columns:
            df.rename(columns={"Title": "Film"}, inplace=True)
        break

if df is None:
    raise ValueError(
        "Could not find the expected table on the Wikipedia page with 'Film' and 'Worldwide gross' columns."
    )

# --- Data Cleaning ---
# Make a copy to avoid SettingWithCopyWarning
df = df.copy()

# Clean 'Worldwide gross' column
# Remove currency symbols, commas, and reference notes like [N 1]
if "Worldwide gross" in df.columns:
    df["Worldwide gross"] = (
        df["Worldwide gross"].astype(str).str.replace("$", "", regex=False)
    )
    df["Worldwide gross"] = df["Worldwide gross"].str.replace(",", "", regex=False)
    df["Worldwide gross"] = (
        df["Worldwide gross"].apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
    )
    df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

# Clean 'Year' column if present
if "Year" in df.columns:
    df["Year"] = (
        df["Year"].astype(str).apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
    )
    df["Year"] = pd.to_numeric(df["Year"], errors="coerce")

# Clean 'Rank' column
if "Rank" in df.columns:
    df["Rank"] = (
        df["Rank"].astype(str).apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
    )
    df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")

# Clean 'Peak' column
if "Peak" in df.columns:
    df["Peak"] = (
        df["Peak"].astype(str).apply(lambda x: re.sub(r"\[.*?\]", "", x)).str.strip()
    )
    df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

# Drop rows where essential numeric columns became NaN after coercion
df.dropna(subset=["Worldwide gross", "Year", "Rank"], inplace=True)

# Save the final dataset
df.to_csv(DATA_FILE_PATH, index=False)

# --- Generate metadata ---
metadata_content = io.StringIO()

metadata_content.write(f"Path to data file: {DATA_FILE_PATH}\n")
metadata_content.write(
    f"Description: Highest-grossing films data scraped from Wikipedia.\n\n"
)

metadata_content.write("Output of df.info():\n")
df.info(buf=metadata_content)
metadata_content.write("\n")

metadata_content.write("Column names:\n")
metadata_content.write(str(df.columns.tolist()) + "\n\n")

metadata_content.write("First few rows (df.head()):\n")
metadata_content.write(df.head().to_string() + "\n\n")

# Explicit ANSWER_FORMAT from user's question
metadata_content.write("ANSWER_FORMAT: JSON array of strings containing the answer.\n")

# Write metadata to file
with open(METADATA_FILE_PATH, "w") as f:
    f.write(metadata_content.getvalue())

----------------------------------------

[2025-08-13 17:46:34]
‚ùå Error during code execution:
Traceback (most recent call last):
  File "/home/charmika/TDS_project2/task_engine.py", line 48, in run_python_code
    execute_code()
  File "/home/charmika/TDS_project2/task_engine.py", line 26, in execute_code
    exec(code, exec_globals)
  File "<string>", line 31, in <module>
AttributeError: 'int' object has no attribute 'lower'

----------------------------------------
