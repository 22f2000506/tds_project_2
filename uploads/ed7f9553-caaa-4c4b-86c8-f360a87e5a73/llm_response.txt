{
    "code": "import pandas as pd\nimport os\nimport re\nimport io\n\n# Define the URL and output paths\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\noutput_dir = \"uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73\"\noutput_csv_path = os.path.join(output_dir, \"data.csv\")\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read tables from the URL\ntables = pd.read_html(url)\n\n# Find the correct table. It should contain 'Film', 'Worldwide gross', 'Year', 'Rank', 'Peak'.\ndf = None\nfor table in tables:\n    if all(col in table.columns for col in ['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak']):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page.\")\n\n# Clean the 'Worldwide gross' column\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str)\ndf['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)) # Remove [citations]\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace('$', '', regex=False).str.replace(',', '', regex=False)\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean 'Rank' and 'Peak' columns\n# Replace non-numeric values (like '\u2014') and convert to numeric\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\ndf['Peak'] = df['Peak'].astype(str).replace('\u2014', pd.NA) # Replace em-dash with NA\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n# Ensure 'Year' is numeric\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Save the cleaned DataFrame to CSV\ndf.to_csv(output_csv_path, index=False)\n\n# Generate metadata\nmetadata_content = io.StringIO()\nmetadata_content.write(\"Output of df.info():\\n\")\ndf.info(buf=metadata_content)\nmetadata_content.write(\"\\n\\nColumn names:\\n\")\nmetadata_content.write(str(df.columns.tolist()))\nmetadata_content.write(\"\\n\\nFirst few rows (df.head()):\\n\")\nmetadata_content.write(str(df.head().to_string())) # Use to_string to avoid truncation\nmetadata_content.write(\"\\n\\nANSWER_FORMAT:\\n\")\nmetadata_content.write(\"JSON array of strings containing the answer.\") # As specified in question.txt\n\nwith open(metadata_path, \"w\") as f:\n    f.write(metadata_content.getvalue())\n",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0KG...\" under 100,000 bytes."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport re\nfrom io import StringIO\n\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\noutput_dir = \"uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Fetch HTML content\ntry:\n    response = requests.get(url)\n    response.raise_for_status() # Raise an exception for bad status codes\nexcept requests.exceptions.RequestException as e:\n    raise SystemExit(f\"Failed to retrieve the webpage: {e}\")\n\n# Use pandas to read HTML tables\n# It returns a list of dataframes found on the page\nlist_of_dfs = pd.read_html(response.text)\n\ndf = None\n# Iterate through the list of dataframes to find the correct one\n# The main table should have 'Film' and 'Worldwide gross' columns\nfor temp_df in list_of_dfs:\n    if 'Film' in temp_df.columns and 'Worldwide gross' in temp_df.columns:\n        df = temp_df\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page with 'Film' and 'Worldwide gross' columns.\")\n\n# Clean the DataFrame\n# Remove 'Reference(s)' column if it exists and is not relevant for analysis\nif 'Reference(s)' in df.columns:\n    df = df.drop(columns=['Reference(s)'])\n\n# Clean 'Worldwide gross' column\n# Remove '$', ',', citation brackets (e.g., [1]), and anything in parentheses\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str).apply(lambda x: re.sub(r'[$,\\[\\]]', '', x))\ndf['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: re.sub(r'\\s*\\([^)]*\\)', '', x))\n# Convert to numeric, coercing errors to NaN\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean 'Year' column\n# Extract the first four digits as the year\ndf['Year'] = df['Year'].astype(str).apply(lambda x: re.search(r'\\d{4}', x).group(0) if re.search(r'\\d{4}', x) else None)\n# Convert to numeric (nullable integer) after cleaning\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n\n# Clean 'Rank' column\n# Convert to numeric, coercing errors to NaN, then to nullable integer\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce').astype('Int64')\n\n# Drop rows where essential columns are NaN after cleaning (e.g., header rows, invalid data)\ndf.dropna(subset=['Rank', 'Worldwide gross', 'Year', 'Film'], inplace=True)\n\n# Ensure data types are correct after cleaning and dropping NaNs for non-nullable use\ndf['Rank'] = df['Rank'].astype(int)\ndf['Year'] = df['Year'].astype(int)\n\n# Sort by Rank to ensure consistent order\ndf = df.sort_values(by='Rank').reset_index(drop=True)\n\n# Save the DataFrame to CSV\ncsv_path = os.path.join(output_dir, \"data.csv\")\ndf.to_csv(csv_path, index=False)\n\n# Generate metadata.txt\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\nwith open(metadata_path, 'w') as f:\n    f.write(\"Output of df.info():\\n\")\n    buffer = StringIO()\n    df.info(buf=buffer)\n    f.write(buffer.getvalue())\n    f.write(\"\\n\\nColumn names:\\n\")\n    f.write(str(df.columns.tolist()))\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string()) # Use .to_string() for better formatting in file\n    f.write(\"\\n\\nANSWER_FORMAT:\\n\")\n    f.write(\"JSON array of strings containing the answer.\")\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 1"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\nimport re\n\n# Define paths\nOUTPUT_DIR = 'uploads/ed7f9553-caaa-4c4b-86c8-f360a87e5a73'\nDATA_FILE_PATH = os.path.join(OUTPUT_DIR, 'data.csv')\nMETADATA_FILE_PATH = os.path.join(OUTPUT_DIR, 'metadata.txt')\nURL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Fetch HTML content\nresponse = requests.get(URL)\nresponse.raise_for_status() # Raise an exception for bad status codes\n\n# Use pandas to read HTML tables\n# It returns a list of DataFrames found on the page\ntables = pd.read_html(response.text)\n\ndf = None\n# Iterate through tables to find the one with the expected columns\n# The column names might vary slightly, e.g., 'Film' or 'Title', 'Worldwide gross', 'Year', 'Rank', 'Peak'\nexpected_col_keywords = ['film', 'worldwide gross']\n\nfor current_df in tables:\n    # Normalize column names for easier checking (case-insensitive)\n    normalized_cols = [col.lower() for col in current_df.columns]\n\n    # Check if key columns are present\n    if all(keyword in normalized_cols for keyword in expected_col_keywords):\n        df = current_df\n        # Standardize column names if variations exist (e.g., 'Title' to 'Film')\n        if 'Title' in df.columns and 'Film' not in df.columns:\n            df.rename(columns={'Title': 'Film'}, inplace=True)\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page with 'Film' and 'Worldwide gross' columns.\")\n\n# --- Data Cleaning ---\n# Make a copy to avoid SettingWithCopyWarning\ndf = df.copy()\n\n# Clean 'Worldwide gross' column\n# Remove currency symbols, commas, and reference notes like [N 1]\nif 'Worldwide gross' in df.columns:\n    df['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].str.replace(',', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\n    df['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean 'Year' column if present\nif 'Year' in df.columns:\n    df['Year'] = df['Year'].astype(str).apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\n    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Clean 'Rank' column\nif 'Rank' in df.columns:\n    df['Rank'] = df['Rank'].astype(str).apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\n    df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n# Clean 'Peak' column\nif 'Peak' in df.columns:\n    df['Peak'] = df['Peak'].astype(str).apply(lambda x: re.sub(r'\\[.*?\\]', '', x)).str.strip()\n    df['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n# Drop rows where essential numeric columns became NaN after coercion\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank'], inplace=True)\n\n# Save the final dataset\ndf.to_csv(DATA_FILE_PATH, index=False)\n\n# --- Generate metadata ---\nmetadata_content = io.StringIO()\n\nmetadata_content.write(f\"Path to data file: {DATA_FILE_PATH}\\n\")\nmetadata_content.write(f\"Description: Highest-grossing films data scraped from Wikipedia.\\n\\n\")\n\nmetadata_content.write(\"Output of df.info():\\n\")\ndf.info(buf=metadata_content)\nmetadata_content.write(\"\\n\")\n\nmetadata_content.write(\"Column names:\\n\")\nmetadata_content.write(str(df.columns.tolist()) + \"\\n\\n\")\n\nmetadata_content.write(\"First few rows (df.head()):\\n\")\nmetadata_content.write(df.head().to_string() + \"\\n\\n\")\n\n# Explicit ANSWER_FORMAT from user's question\nmetadata_content.write(\"ANSWER_FORMAT: JSON array of strings containing the answer.\\n\")\n\n# Write metadata to file\nwith open(METADATA_FILE_PATH, 'w') as f:\n    f.write(metadata_content.getvalue())\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0KG...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 2"
}