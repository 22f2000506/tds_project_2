{
    "code": "import os\nimport io\nimport requests\nimport pandas as pd\n\n# Define the directory for uploads\noutput_dir = 'uploads/3dad984e-7076-4464-9345-ec0c6a5c8ffd'\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Read tables from the URL\ntables = pd.read_html(url)\n\n# The highest-grossing films table is usually the first significant table on the page.\ndf = tables[0].copy()\n\n# Clean column names: remove any bracketed references (e.g., [N 1]) and strip whitespace\ndf.columns = df.columns.astype(str)\ndf.columns = df.columns.str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n\n# Define the required columns for the analysis based on the questions\nrequired_cols = ['Rank', 'Peak', 'Film', 'Worldwide gross', 'Year']\n\n# Filter the DataFrame to only include these specific columns, ensuring they exist\ndf = df[required_cols]\n\n# Data Cleaning:\n\n# 1. Clean 'Worldwide gross' column:\n#    - Remove '$' and ',' characters.\n#    - Convert to numeric, coercing errors (e.g., 'TBA', '\u2013') to NaN.\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# 2. Clean 'Rank' column:\n#    - Remove any non-digit characters (e.g., '\u2014' or empty strings).\n#    - Convert to numeric, coercing errors to NaN.\ndf['Rank'] = df['Rank'].astype(str).str.replace(r'[^\\d]+', '', regex=True)\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n# 3. Clean 'Peak' column:\n#    - Convert to numeric, coercing errors to NaN.\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n# 4. Clean 'Year' column:\n#    - Convert to numeric, coercing errors to NaN.\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Drop rows where critical columns (needed for analysis) have NaN values after cleaning\ndf.dropna(subset=['Worldwide gross', 'Rank', 'Year', 'Peak'], inplace=True)\n\n# Ensure 'Year', 'Rank', 'Peak' are integers\ndf['Year'] = df['Year'].astype(int)\ndf['Rank'] = df['Rank'].astype(int)\ndf['Peak'] = df['Peak'].astype(int)\n\n# Save the cleaned DataFrame to CSV\ndata_csv_path = os.path.join(output_dir, 'data.csv')\ndf.to_csv(data_csv_path, index=False)\n\n# Generate metadata.txt\nmetadata_txt_path = os.path.join(output_dir, 'metadata.txt')\nwith open(metadata_txt_path, 'w') as f:\n    f.write(\"Output of df.info():\\n\")\n    buffer = io.StringIO()\n    df.info(buf=buffer)\n    f.write(buffer.getvalue())\n    f.write(\"\\n\\nColumn names:\\n\")\n    f.write(\", \".join(df.columns.tolist()))\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\nANSWER_FORMAT: JSON\\n\")",
    "libraries": [
        "requests",
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport os\n\n# Define paths\noutput_dir = 'uploads/3dad984e-7076-4464-9345-ec0c6a5c8ffd'\ndata_filepath = os.path.join(output_dir, 'data.csv')\nmetadata_filepath = os.path.join(output_dir, 'metadata.txt')\n\n# Create the output directory if it does not exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\n# Read tables from the URL\ntables = pd.read_html(url)\n\n# Find the correct table. Look for a table that likely contains film data.\n# The main table usually has columns like 'Film', 'Worldwide gross', 'Year'.\n# Iterate through tables and check for relevant column names.\nmain_df = None\nfor i, df_temp in enumerate(tables):\n    # Clean column names for robust checking\n    df_temp.columns = df_temp.columns.astype(str).str.replace(r'\\[.*\\]', '', regex=True).str.strip()\n    if all(col in df_temp.columns for col in ['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak']):\n        main_df = df_temp\n        break\n\nif main_df is None:\n    raise ValueError(\"Could not find the main highest-grossing films table.\")\n\n# Clean column names again thoroughly, just in case (e.g., footnotes)\nmain_df.columns = main_df.columns.astype(str).str.replace(r'\\[.*\\]', '', regex=True).str.strip()\n\n# Clean up data types and values\n# For 'Worldwide gross', remove '$' and ',' and convert to numeric\n# Some cells might contain multiple values (e.g., 'Various' or ranges), handle errors by coercing to NaN\nmain_df['Worldwide gross'] = main_df['Worldwide gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n# Remove any non-numeric characters from 'Worldwide gross' before converting, e.g. 'est.'\nmain_df['Worldwide gross'] = main_df['Worldwide gross'].str.extract(r'([\\d.]+)').astype(float)\n\n# For 'Year', convert to numeric\nmain_df['Year'] = pd.to_numeric(main_df['Year'], errors='coerce')\n\n# For 'Rank' and 'Peak', convert to numeric\nmain_df['Rank'] = pd.to_numeric(main_df['Rank'], errors='coerce')\nmain_df['Peak'] = pd.to_numeric(main_df['Peak'], errors='coerce')\n\n# Drop rows where essential columns are NaN after cleaning\nmain_df.dropna(subset=['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Save the DataFrame to CSV\nmain_df.to_csv(data_filepath, index=False)\n\n# Generate metadata.txt\nwith open(metadata_filepath, 'w') as f:\n    f.write(f\"Output of df.info():\\n\")\n    main_df.info(buf=f)\n    f.write(f\"\\nColumn names:\\n{', '.join(main_df.columns)}\\n\")\n    f.write(f\"\\nFirst few rows (df.head()):\\n{main_df.head().to_string()}\\n\")\n    f.write(\"\\nANSWER_FORMAT:\\nJSON array of strings containing the answer\")\n",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\n\n# Define paths\noutput_dir = 'uploads/3dad984e-7076-4464-9345-ec0c6a5c8ffd'\ndata_csv_path = os.path.join(output_dir, 'data.csv')\nmetadata_txt_path = os.path.join(output_dir, 'metadata.txt')\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\ntry:\n    # Use pandas to read HTML tables directly. The 'match' argument helps to find the specific table.\n    # The table caption or a string within the table can be used for matching.\n    tables = pd.read_html(url, match='Highest-grossing films')\n    df = tables[0]\n\n    # Clean column names by removing bracketed references like '[n]' and stripping whitespace\n    df.columns = df.columns.astype(str).str.replace(r'\\\\[.*\\\\]', '', regex=True).str.strip()\n\n    # Data Cleaning:\n    # 1. Clean 'Worldwide gross' column: remove '$' and ',' and convert to numeric (float)\n    if 'Worldwide gross' in df.columns:\n        df['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n        # Handle cases where values might be ranges or have other non-numeric characters before conversion\n        # For instance, if 'Worldwide gross' is 'N/A' or '?' it should become NaN, then fill with 0\n        df['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce').fillna(0)\n\n    # 2. Clean 'Year' column: convert to integer. Handle potential non-numeric entries.\n    if 'Year' in df.columns:\n        df['Year'] = pd.to_numeric(df['Year'], errors='coerce').fillna(0).astype(int)\n\n    # 3. Clean 'Rank' and 'Peak' columns: convert to integer. Handle potential non-numeric entries.\n    if 'Rank' in df.columns:\n        df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce').fillna(0).astype(int)\n    if 'Peak' in df.columns:\n        df['Peak'] = pd.to_numeric(df['Peak'], errors='coerce').fillna(0).astype(int)\n\n    # Save the cleaned DataFrame to a CSV file\n    df.to_csv(data_csv_path, index=False)\n\n    # Generate metadata.txt\n    with open(metadata_txt_path, 'w') as f:\n        f.write('Output of df.info():\\n')\n        df.info(buf=f)\n        f.write('\\nColumn names:\\n')\n        f.write(str(df.columns.tolist()))\n        f.write('\\n\\nFirst few rows (df.head()):\\n')\n        f.write(df.head().to_string())\n        f.write('\\n\\nANSWER_FORMAT: JSON')\n\nexcept Exception as e:\n    # Log any errors that occur during the process\n    with open(metadata_txt_path, 'w') as f:\n        f.write(f\"Error during data extraction: {e}\")\n    print(f\"An error occurred: {e}\")\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 1"
}