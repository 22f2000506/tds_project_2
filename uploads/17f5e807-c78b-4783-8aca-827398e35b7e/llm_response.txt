{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\n\n# Define paths\noutput_dir = \"uploads/17f5e807-c78b-4783-8aca-827398e35b7e\"\ndata_csv_path = os.path.join(output_dir, \"data.csv\")\nmetadata_txt_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL to scrape\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch the HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an exception for bad status codes\n\n# Read tables into a list of DataFrames\ntables = pd.read_html(response.text)\n\n# Identify the correct table. The first table is typically the 'Highest-grossing films (worldwide)'.\ndf = None\nfor table in tables:\n    # Look for common columns that identify the main film list table\n    if 'Film' in table.columns and 'Worldwide gross' in table.columns and 'Year' in table.columns:\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the highest-grossing films table on the Wikipedia page.\")\n\n# Standardize column names by removing any potential footnote markers (e.g., [A], [1])\ndf.columns = df.columns.astype(str).str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n\n# Data Cleaning\n# Clean 'Worldwide gross' column\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str)\n# Remove '$', ',', and any bracketed references (e.g., [1], [A], [E])\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace('$', '', regex=False).str.replace(',', '', regex=False)\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce') # Coerce errors to NaN\n\n# Clean 'Year' column\ndf['Year'] = df['Year'].astype(str).str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64') # Use Int64 for nullable integer\n\n# Clean 'Rank' column\ndf['Rank'] = df['Rank'].astype(str).str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce').astype('Int64')\n\n# Clean 'Peak' column\ndf['Peak'] = df['Peak'].astype(str).str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce').astype('Int64')\n\n# Drop rows where 'Worldwide gross' or 'Year' are NaN after cleaning, as they are crucial for analysis\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Select and reorder columns for the final CSV, focusing on those relevant for the questions\nfinal_df = df[['Rank', 'Peak', 'Film', 'Worldwide gross', 'Year']]\n\n# Save the final dataset to CSV\nfinal_df.to_csv(data_csv_path, index=False)\n\n# Generate metadata.txt\nwith open(metadata_txt_path, 'w') as f:\n    f.write(\"Output of df.info():\\n\")\n    buffer = io.StringIO()\n    final_df.info(buf=buffer)\n    f.write(buffer.getvalue())\n    f.write(\"\\n\\n\")\n\n    f.write(\"Column names:\\n\")\n    f.write(str(final_df.columns.tolist()))\n    f.write(\"\\n\\n\")\n\n    f.write(\"First few rows (df.head()):\\n\")\n    f.write(final_df.head().to_string())\n    f.write(\"\\n\\n\")\n\n    f.write(\"ANSWER_FORMAT: JSON\\n\")\n",
    "libraries": [
        "pandas",
        "requests",
        "beautifulsoup4"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\n\n# Define paths\nOUTPUT_DIR = \"uploads/17f5e807-c78b-4783-8aca-827398e35b7e\"\nDATA_FILE = os.path.join(OUTPUT_DIR, \"data.csv\")\nMETADATA_FILE = os.path.join(OUTPUT_DIR, \"metadata.txt\")\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# URL to scrape\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Scrape the table using pandas\n# The table of interest is usually the first or second one.\n# We look for a table containing 'Worldwide gross' and 'Year'.\ntry:\n    tables = pd.read_html(url)\n    df = None\n    for table in tables:\n        if 'Worldwide gross' in table.columns and 'Year' in table.columns and 'Title' in table.columns:\n            df = table\n            break\n\n    if df is None:\n        raise ValueError(\"Could not find the expected table on the Wikipedia page.\")\n\n    # Select relevant columns and rename for consistency if needed\n    # Ensure column names are exactly as expected for cleaning\n    df = df[['Rank', 'Peak', 'Title', 'Worldwide gross', 'Year']].copy()\n\n    # Clean 'Worldwide gross' column\n    # Remove '$', ',', and text within square brackets (e.g., '[A]')\n    df['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].str.replace(',', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].str.replace(r'\\[.*\\]', '', regex=True).str.strip()\n    df['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n    # Clean 'Rank' and 'Peak' columns\n    # Convert to numeric, coercing errors to NaN\n    df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n    df['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n    # Convert 'Year' to numeric\n    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n    # Drop rows where essential numeric columns became NaN due to cleaning issues\n    df.dropna(subset=['Rank', 'Peak', 'Worldwide gross', 'Year'], inplace=True)\n\n    # Save the cleaned DataFrame to CSV\n    df.to_csv(DATA_FILE, index=False)\n\n    # Generate metadata\n    with open(METADATA_FILE, 'w') as f:\n        f.write(\"Output of df.info():\\n\")\n        df.info(buf=f)\n        f.write(\"\\n\\nColumn names:\\n\")\n        f.write(str(df.columns.tolist()))\n        f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n        f.write(df.head().to_string())\n        f.write(\"\\n\\nANSWER_FORMAT: JSON\\n\")\n\nexcept Exception as e:\n    with open(METADATA_FILE, 'w') as f:\n        f.write(f\"Error during data collection: {e}\\n\")\n    raise\n",
    "libraries": [
        "pandas",
        "requests",
        "lxml"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}