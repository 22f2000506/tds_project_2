{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\nimport re\n\n# Define paths\nOUTPUT_DIR = 'uploads/8d07eb91-ace0-4864-9b41-8765eb26f61e'\nDATA_PATH = os.path.join(OUTPUT_DIR, 'data.csv')\nMETADATA_PATH = os.path.join(OUTPUT_DIR, 'metadata.txt')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# URL of the Wikipedia page\nURL = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\n# Fetch the HTML content from the URL\nresponse = requests.get(URL)\nresponse.raise_for_status() # Raise an exception for HTTP errors\nhtml_content = response.text\n\n# Read HTML tables into a list of DataFrames\ntables = pd.read_html(html_content)\n\n# Identify the \"Timeline of highest-grossing films\" table\n# This table usually contains 'Year', 'Title', 'Worldwide gross', 'Peak position'\ndf = None\nfor table in tables:\n    if all(col in table.columns for col in ['Year', 'Worldwide gross', 'Peak position']):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the 'Timeline of highest-grossing films' table on the Wikipedia page.\")\n\n# Rename 'Peak position' to 'Peak' and 'Title' to 'Film'\ndf = df.rename(columns={'Peak position': 'Peak', 'Title': 'Film'})\n\n# Clean 'Worldwide gross' column\n# Remove non-numeric characters (except period) and convert to float\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace(r'[^0-9.]', '', regex=True)\n# Some rows might have empty strings after cleaning if they were non-numeric or footnote references, convert them to NaN\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n# Clean 'Peak' column\n# Extract only digits and convert to integer, handling potential footnotes (e.g., '1[note 1]')\ndf['Peak'] = df['Peak'].astype(str).str.extract(r'(\\d+)').astype(int)\n\n# Create 'Rank' column based on 'Worldwide gross' (descending rank)\n# Use 'min' method to assign the same rank to ties\ndf['Rank'] = df['Worldwide gross'].rank(ascending=False, method='min')\n# Convert Rank to integer, as it typically is an integer value.\ndf['Rank'] = df['Rank'].astype(int)\n\n# Select and reorder columns to match general request and ensure required columns are present\ndf = df[['Rank', 'Film', 'Worldwide gross', 'Year', 'Peak']]\n\n# Save the DataFrame to a CSV file\ndf.to_csv(DATA_PATH, index=False)\n\n# Generate metadata.txt\nmetadata_content = io.StringIO()\n\nmetadata_content.write(\"Output of df.info():\\n\")\ndf.info(buf=metadata_content)\nmetadata_content.write(\"\\n\\n\")\n\nmetadata_content.write(\"Column names:\\n\")\nmetadata_content.write(str(df.columns.tolist()))\nmetadata_content.write(\"\\n\\n\")\n\nmetadata_content.write(\"First few rows (df.head()):\\n\")\nmetadata_content.write(df.head().to_string())\nmetadata_content.write(\"\\n\\n\")\n\nmetadata_content.write(\"ANSWER_FORMAT: JSON\\n\")\n\n# Add file paths and descriptions\nmetadata_content.write(\"\\n\")\nmetadata_content.write(f\"File: {DATA_PATH}\\n\")\nmetadata_content.write(\"Description: Cleaned and processed data containing highest-grossing films from the Wikipedia timeline table, including derived Rank and original Peak position.\\n\")\n\n# Write metadata to file\nwith open(METADATA_PATH, 'w', encoding='utf-8') as f:\n    f.write(metadata_content.getvalue())\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\n\n# Define the URL and output directory\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\noutput_dir = \"uploads/8d07eb91-ace0-4864-9b41-8765eb26f61e\"\ndata_path = os.path.join(output_dir, \"data.csv\")\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Fetch the HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an exception for bad status codes\n\n# Read tables from the HTML\ntables = pd.read_html(response.text)\n\n# Attempt to find the correct table: the main list of highest-grossing films.\n# It's usually the first comprehensive table containing 'Film', 'Worldwide gross', 'Year', 'Rank', 'Peak'.\ndf = None\nfor table in tables:\n    # Create a copy to avoid SettingWithCopyWarning during column cleaning\n    temp_df = table.copy()\n    \n    # Clean column names by removing annotations like '[A]' and leading/trailing spaces\n    temp_df.columns = [col.split('[')[0].strip() for col in temp_df.columns]\n    \n    # Check if the table contains the expected core columns\n    expected_cols = ['Film', 'Worldwide gross', 'Year', 'Rank', 'Peak']\n    if all(col in temp_df.columns for col in expected_cols):\n        df = temp_df\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the main 'Highest-grossing films' table with expected columns on the Wikipedia page.\")\n\n# --- Data Cleaning ---\n\n# Further clean column names if necessary (e.g., removing ' (estimated)')\ndf.columns = [col.replace(' (estimated)', '').strip() for col in df.columns]\n\n# Clean and convert relevant columns to numeric\nfor col_name in ['Worldwide gross', 'Year', 'Rank', 'Peak']:\n    if col_name in df.columns:\n        # Convert to string, remove text in brackets, strip spaces\n        df[col_name] = df[col_name].astype(str).str.split('[').str[0].str.strip()\n        if col_name == 'Worldwide gross':\n            # Remove '$' and ',' from worldwide gross values\n            df[col_name] = df[col_name].str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n        # Convert to numeric, coercing errors to NaN\n        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n\n# Drop rows where essential columns are NaN after cleaning\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Save the cleaned DataFrame to CSV\ndf.to_csv(data_path, index=False)\n\n# --- Generate Metadata ---\nmetadata_content = []\nmetadata_content.append(\"df.info():\\n\")\n# Capture df.info() output\nbuffer = io.StringIO()\ndf.info(buf=buffer)\nmetadata_content.append(buffer.getvalue())\nmetadata_content.append(\"\\nColumn names:\\n\")\nmetadata_content.append(str(df.columns.tolist()))\nmetadata_content.append(\"\\n\\nFirst few rows (df.head()):\\n\")\nmetadata_content.append(df.head().to_string())\nmetadata_content.append(\"\\n\\nANSWER_FORMAT: JSON array of strings\")\n\nwith open(metadata_path, \"w\") as f:\n    f.write(\"\\n\".join(metadata_content))\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0K...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\n\n# Create the output directory if it doesn't exist\noutput_dir = 'uploads/8d07eb91-ace0-4864-9b41-8765eb26f61e'\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch the HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n\n# Read HTML tables into a list of DataFrames\ntables = pd.read_html(response.text)\n\n# Identify the correct table. It should have 'Rank', 'Film', 'Worldwide gross', 'Year', and 'Peak rank'.\ndf = None\nrequired_cols = ['Rank', 'Film', 'Worldwide gross', 'Year', 'Peak rank']\n\nfor table in tables:\n    # Normalize column names by removing leading/trailing spaces and newlines\n    table.columns = table.columns.str.strip().str.replace('\\n', ' ', regex=False)\n    if all(col in table.columns for col in required_cols):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(f\"Could not find the main table with expected columns: {required_cols}.\")\n\n# Select only the relevant columns to avoid extra columns from Wikipedia tables\ndf = df[required_cols].copy()\n\n# Data Cleaning\n# Clean 'Worldwide gross' column: remove '$', ',', and reference notes like '[a]'\ndf['Worldwide gross'] = df['Worldwide gross'].astype(str)\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace(r'\\[.*?\\]', '', regex=True) # Remove references\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace('$', '', regex=False) # Remove dollar sign\ndf['Worldwide gross'] = df['Worldwide gross'].str.replace(',', '', regex=False) # Remove commas\ndf['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce') # Convert to numeric\n\n# Clean 'Year' column: ensure it's numeric\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Clean 'Rank' column: ensure it's numeric\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n# Clean 'Peak rank' column: it might contain text like '1 (1 time)'\n# Extract just the number at the beginning of the string\ndf['Peak rank'] = df['Peak rank'].astype(str).str.extract(r'(\\d+)').astype(float)\n\n# Drop rows where critical columns are NaN after cleaning (e.g., gross, year, rank, peak rank)\ndf.dropna(subset=['Worldwide gross', 'Year', 'Rank', 'Peak rank'], inplace=True)\n\n# Save the cleaned DataFrame to CSV\ndata_csv_path = os.path.join(output_dir, 'data.csv')\ndf.to_csv(data_csv_path, index=False)\n\n# Generate metadata.txt\nmetadata_path = os.path.join(output_dir, 'metadata.txt')\n\nwith open(metadata_path, 'w') as f:\n    f.write(\"Output of df.info():\\n\")\n    buffer = io.StringIO()\n    df.info(buf=buffer)\n    f.write(buffer.getvalue())\n    f.write(\"\\n\\nColumn names:\\n\")\n    f.write(str(df.columns.tolist()))\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\nANSWER_FORMAT: JSON array of strings containing the answer.\")\n",
    "libraries": [
        "pandas",
        "requests"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0K...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 1"
}